FROM ubuntu:latest AS base

USER root
ARG spark_version="3.5.5"
ARG openjdk_version="17"
ARG python_version="3.12"
ENV workdir="/daprep"

# Set WorkDir
WORKDIR $workdir

RUN apt-get update && apt-get install -y openjdk-${openjdk_version}-jdk && \
    rm -rf /var/lib/apt/lists/*

# Install Spark
RUN \
    apt-get update && \
    apt-get install -y wget && \
    wget https://archive.apache.org/dist/spark/spark-${spark_version}/spark-${spark_version}-bin-hadoop3.tgz && \
    tar xvf spark-${spark_version}-bin-hadoop3.tgz && \
    mv spark-${spark_version}-bin-hadoop3/ /opt/spark && \
    rm spark-${spark_version}-bin-hadoop3.tgz
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYSPARK_PYTHON=/usr/bin/python3


# Install Python in new layer to improve rebuilds with different versions
RUN apt-get update && \
    apt-get install -y python${python_version} python3-pip && \
    rm -rf /var/lib/apt/lists/*

# Copy Requirements
COPY requirements.txt .

# Install Python Dependencys
RUN python3 -m pip config set global.break-system-packages true
RUN  pip install -r requirements.txt

CMD ["/bin/bash"]

USER 185