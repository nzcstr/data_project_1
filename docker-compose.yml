

services:
  # Based on Chris instructions
#  spark-py:
#    image: docker.io/bitnami/spark:latest
#    container_name: spark_container
#    command: /opt/spark/bin/pyspark
#    environment:
#      - SPARK_MODE=master
#    ports:
#      - "7077:7077"
#      - "8080:8080"
#    volumes:
#      - .venv:/spark_py_env/
#    restart: always

# Based on GPT instructions OPT A: Works and I can connect to the interpreter. Can run python commands in IDE
#  spark:
#    image: apache/spark-py:latest
#    container_name: pyspark_dev
#    command: /bin/bash  # Start with a shell for flexibility
#    volumes:
#      - .:/workspace # Mount the current directory
#    working_dir: /workspace
#    environment:
#      - PYSPARK_PYTHON=python3
#      - SPARK_HOME=/opt/spark
# BASED ON GPT instructions OPT B: Requires a custom image (See Dockerfile). It allows me to specify the python
#  libraries I need directly in the Dockerfile. You need to "build" the image first from the Dockerfile
#  spark:
#    build: .
#    container_name: pyspark_dev
#    tty: true
#    stdin_open: true
#    volumes:
#      - .:/workspace
#    environment:
#      - PYTHONPATH=/opt/venv/lib/python3.10/site-packages
#      - SPARK_HOME=/opt/spark
#      - PATH=/opt/venv/bin:$PATH
##    ports:
##      - "4040:4040"
##      - "8888:8888"
#    command: /bin/bash
  ### ------####
#  python_spark_dev:   # Use this if you need to build the image
#    platform: linux/amd64
#    build:
#      context: .
#      dockerfile: Dockerfile.dev
#    container_name: python_spark_dev
#    image: nzcstr/pyspark:v2
#    tty: true # Keep container running
#    environment: # Temporal solution. ALready corrected in dockerfile.dev
#      - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
#      - PATH=/usr/lib/jvm/java-11-openjdk-amd64/bin/java:${PATH}


#  python_spark_v2:
#    build:
#      context: .
#      dockerfile: Dockerfile.dev2
#    container_name: python_spark_dev2
#    image: nzcstr/pyspark:v3
#    tty: true

#  python_spark_dev_prebuilt:
#    image: my_image_pyspark

  pyspark_chris:
    build:
      context: .
      dockerfile: Dockerfile.dev3
    container_name: pyspark_container
    #image: chris/pyspark:v1
    tty: true
    ports:
      - "4040:4040"
      - "7077:7077"
      - "8080:8080"
    volumes:
      - /Users/nzcstr/Documents/DATA_projects/data_project_1/:/opt/project/
    networks:
     - my_network

  mongodb:
    image: mongo:latest # Necessary for compatibility issues. Mongo v5.+ requires CPU with AVX support.
    container_name: mongodb_container
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    restart: always
    environment:
      - MONGO_HOME=/usr/bin/
      - PATH=/usr/bin/mongo:$PATH
    networks:
      - my_network

  postgres:
    image: postgres:latest
    container_name: postgres_container
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: netflix_show_recommendation
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    restart: always
    networks:
      - my_network
  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: adminpassword
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - my_network
volumes:
  mongo_data:
  pg_data:

networks:
  my_network:

